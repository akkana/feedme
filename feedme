#!/usr/bin/env python

# feedme: read RSS/Atom feeds and convert to Plucker files.
# Copyright 2009,2011 Akkana Peck <akkana@shallowsky.com>
# Based on feedread, Copyright (C) 2009 Benjamin M. A'Lee <bma@subvert.org.uk>
# 
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
# 
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details:
# <http://www.gnu.org/licenses/>.

# Goals for feedme 0.9: use real HTML parsing, not rexexp; add image fetching

ConfigHelp = """Configuration options:

Configuration options most useful in a DEFAULT section,
applicable to all feeds:
  ascii
    Convert all pages to plain ASCII. Useful for reading devices like Palm
    that can't display other character sets reliably.
  dir
    Where to save the collected pages.
    See save_days for how long they will be kept.
  formats
    Comma-separated list of output formats.
    Default "none", which will result in HTML output.
    Other options: epub, fb2, plucker.
  logfile
    Save output (including debugging) to this log.
  verbose
    Print lots of debugging chatter while feeding.
  min_width
    The minimum number of characters in an item link. Links shorter than this
    will be padded to this length (to make tapping easier). Default 25.
  save_days
    How long to retain feeds locally.

Configuration options you might want to reset for specific feeds:
  continue_on_timeout
    Normally, if one page times out, feedme will assume the site is down.
    On sites that link to content from many different URLs, set this
    to true.
  encoding
    Normally feedme will try to guess the encoding from the page.
    But some pages lie, so use this to override that.
  levels
    Level 1: only save the RSS page.
    Level 2: save sub-pages.
  nocache
    Don't check whether we've seen an entry before: collect everything.
  nonlocal_images
    Normally feedme will ignore images from other domains (usually ads).
    But some sites link to images from all over; set this to true in that case.
  skip_images
    Don't save images. Default true.
  skip_links:
    For sites with levels=1 where you just want a single news feed and
    never want to click on anything (e.g. slashdot), this can eliminate
    distracting links that you might tap on accidentally while scrolling.
  url
    The RSS URL for the site.
  when
    When to check this site, if not every time.
    May be a weekday, e.g. Sat, or a month date, e.g. 1 to check only
    on the first day of any month.
"""

import cPickle
import time
import os, sys
import re
#import types
import shutil
import traceback

import feedparser
import urllib2
import socket
import posixpath

# Our module for parsing HTML inside feeds:
import feedmeparser

has_ununicode=True
try:
    import ununicode
except ImportError, e:
    has_ununicode=False

#
# Clean up old feed directories
#
def clean_up(config):
    try:
        days = int(config.get('DEFAULT', 'save_days'))
        feeddir = config.get('DEFAULT', 'dir')
        feeddir = feedmeparser.sub_tilde(feeddir)
    except:
        print >>sys.stderr, \
            "Error trying to get save_days and feed dir; can't clean up"
        return

    print >>sys.stderr, "Cleaning up anything older than", \
        days, "days from", feeddir

    now = time.time()
    for dir in os.listdir(feeddir):
        d = os.path.join(feeddir, dir)

        # Only clean up directories, not files sitting in the feeds dir.
        if not os.path.isdir(d):
            continue

        try:
            howold = (now - os.path.getctime(d)) / 60 / 60 / 24
            if howold > days:
                print >>sys.stderr, "Deleting", d
                if os.path.isdir(d):
                    shutil.rmtree(d)
                else:
                    os.unlink(d)
        except Exception, e:
            print "Couldn't unlink", d, str(e)

##################################################################
# OUTPUT GENERATING FUNCTIONS
# Define functions for each output format you need to support.
#

def run_conversion_cmd(appargs):
    if True or verbose:
        cmd = " ".join(appargs)
        print >>sys.stderr, "Running:", cmd
        sys.stdout.flush()

    retval = os.spawnvp(os.P_WAIT, appargs[0], appargs)
    #retval = os.system(cmd)
    if retval != 0:
        raise OSError(retval, "Couldn't run: " + ' '.join(appargs))

#
# Generate a Plucker file
#
def make_plucker_file(indexfile, feedname, levels, ascii):
    day = time.strftime("%a")
    docname = day + " " + feedname
    cleanfilename = day + "_" + feedname.replace(" ", "_")

    # Make sure the plucker directory exists:
    pluckerdir = os.path.join(feedmeparser.sub_tilde("~/.plucker"), "feedme")
    if not os.path.exists(pluckerdir):
        os.makedirs(pluckerdir)

    # Run plucker. This should eventually be configurable --
    # but how, with arguments like these?

    # Plucker mysteriously creates unbeamable files if the
    # document name has a colons in it.
    # So use the less pretty but safer underscored docname.
    #docname = cleanfilename
    appargs = [ "plucker-build", "-N", docname,
                "-f", os.path.join("feedme", cleanfilename),
                "--stayonhost", "--noimages",
                "--maxdepth", str(levels),
                "--zlib-compression", "--beamable",
                "-H", "file://" + indexfile ]
    if not ascii:
        appargs.append("--charset=utf-8")

    run_conversion_cmd(appargs)

#
# http://calibre-ebook.com/user_manual/conversion.html
#
def make_calibre_file(indexfile, feedname, extension, levels, ascii,
                      author, flags):
    day = time.strftime("%a")
    # Prepend daynum to the filename because fbreader can only sort by filename
    #daynum = time.strftime("%w")
    cleanfilename = day + "_" + feedname.replace(" ", "_")
    outdir = os.path.join(config.get('DEFAULT', 'dir'), extension[1:])
    if not os.access(outdir, os.W_OK):
        os.makedirs(outdir)

    appargs = [ "ebook-convert",
                indexfile,
                #os.path.join(feedmeparser.sub_tilde("~/feeds"),
                #             cleanfilename + extension),
                # directory should be configurable too, probably
                os.path.join(outdir, cleanfilename + extension),
                "--authors", author ]
    for flag in flags:
        appargs.append(flag)
    if True or verbose:
        cmd = " ".join(appargs)
        print >>sys.stderr, "Running:", cmd
        sys.stdout.flush()

    run_conversion_cmd(appargs)

#
# Generate a fictionbook2 file
#
def make_fb2_file(indexfile, feedname, levels, ascii):
    make_calibre_file(indexfile, feedname, ".fb2", levels, ascii,
                      "feedme", flags = [ "--disable-font-rescaling" ] )

#
# Generate an ePub file
# http://calibre-ebook.com/user_manual/cli/ebook-convert-3.html#html-input-to-epub-output
#
def make_epub_file(indexfile, feedname, levels, ascii):
    make_calibre_file(indexfile, feedname, ".epub", levels, ascii,
                      time.strftime("%m-%d %a") + " feeds",
                      flags = [ '--no-default-epub-cover',
                                '--dont-split-on-page-breaks' ])

# END OUTPUT GENERATING FUNCTIONS
##################################################################

##################################################################
# MsgLog: Print messages and also batch them up to print at the end:
#
class MsgLog:
    def __init__(self):
        self.msgstr = ""
        self.errstr = ""

    def msg(self, s):
        self.msgstr += "\n" + s
        print >>sys.stderr, "MESSAGE:", s.encode('ascii', 'backslashreplace')

    def warn(self, s):
        self.msgstr += "\n" + s
        print  >>sys.stderr,"WARNING:", s.encode('ascii', 'backslashreplace')

    def err(self, s):
        self.errstr += "\n" + s
        print  >>sys.stderr,"ERROR:", s.encode('ascii', 'backslashreplace')
        #traceback.print_stack()

    def get_msgs(self):
        return self.msgstr

    def get_errs(self):
        return self.errstr

# file-like class that can optionally send output to a log file. Inspired by
# http://www.redmountainsw.com/wordpress/archives/python-subclassing-file-types
# and with help from KirkMcDonald.
class tee():
    def __init__(self, _fd1, _fd2):
        self.fd1 = _fd1
        self.fd2 = _fd2

    def __del__(self):
        if self.fd1 != sys.stdout and self.fd1 != sys.stderr:
            self.fd1.close()
        if self.fd2 != sys.stdout and self.fd2 != sys.stderr:
            self.fd2.close()

    def write(self, text):
        if isinstance(text, unicode):
            text = text.encode('utf-8')
        self.fd1.write(text)
        self.fd2.write(text)

    def flush(self):
        self.fd1.flush()
        self.fd2.flush()

#
# Ctrl-C Interrupt handler: prompt for what to do.
#
def handle_keyboard_interrupt(msg):
    # os.isatty() doesn't work, so:
    if not hasattr(sys.stdin, "isatty"):
        print "Interrupt, and not running interactively. Exiting."
        sys.exit(1)

    try:
        response = raw_input(msg)
    except exceptions.EOFError:
        # This happens if we're run from a script rather than interactively
        # and yet someone sends a SIGINT, perhaps because we're timing out
        # and someone logged in to kick us back into operation.
        # In this case, pretend the user typed 'n',
        # meaning skip to next site.
        return 'n'
    if response == '':
        return '\0'
    if response[0] == 'q':
        sys.exit(1)
    return response[0]

def falls_between(when, time1, time2):
    """Does a given day-of-week or day-of-month fall between
       the two given times? It is presumed that time1 <= time2.
       If when == "Tue", did we cross a tuesday getting from time1 to time2?
       If when == 15, did we cross the 15th of a month?
       If when == none, return True.
       If when matches time2, return True.
    """
    if not when or type(when) is str and len(when) <= 0:
        return True

    # We need both times both in seconds since epoch and in struct_time:
    def both_time_types(t):
        """Given a time that might be either seconds since epoch or struct_time,
           return a tuple of (seconds, struct_time).
        """
        if type(t) is time.struct_time:
            return time.mktime(t), t
        elif type(t) is int or type(t) is float:
            return t, time.localtime(t)
        else : raise ValueError("%s not int or struct_time" % str(t))

    (t1, st1) = both_time_types(time1)
    (t2, st2) = both_time_types(time2)

    daysdiff = (t2 - t1) / 60. / 60. / 24.
    if daysdiff < 0:
        msglog.err("daysdiff < 0!!! " + str(daysdiff))

    # Is it a day of the month?
    try:
        day_of_month = int(when)

        # It is a day of the month! How many days in between the two dates?
        if daysdiff > 31:
            return True

        # Now we know the two dates differ by less than a month.
        # Are time1 and time2 both in the same month? Then it's easy.
        if st1.tm_mon == st2.tm_mon:
            return st1.tm_mday <= day_of_month and st2.tm_mday >= day_of_month

        # Else time1 is the month prior to time2, so:
        return st1.tm_mday < day_of_month or day_of_month <= st2.tm_mday

    except ValueError :  # Not an integer, probably a string.
        pass

    if type(when) is not str:
        raise ValueError("%s must be a string or integer" % when)

    # Okay, not a day of the month. Is it a day of the week?
    # We have to start with Monday because struct_time.tm_wday does.
    weekdays = [ 'mo', 'tu', 'we', 'th', 'fr', 'sa', 'su' ]
    if len(when) < 2:
        raise ValueError("%s too short: days must have at least 2 chars" % when)

    when = when[0:2].lower()
    if when not in weekdays:
        raise ValueError("%s is a string but not a day" % when)

    # Whew -- we know it's a day of the week.

    # Has more than a week passed? Then it encompasses all weekdays.
    if daysdiff > 7:
        return True

    day_of_week = weekdays.index(when)
    return (st2.tm_wday - day_of_week) % 7 < daysdiff

#
# Get a single feed
#
def get_feed(feedname, config, cache, cachefile, last_time, msglog):
    """Fetch a single feed"""
    # Mandatory arguments:
    try:
        sitefeedurl = config.get(feedname, 'url')
        feeddir = config.get(feedname, 'dir')
    except:
        msglog.err("Error reading feedme.conf entry for: " + feedname)
        return

    verbose = (config.get(feedname, 'verbose').lower() == 'true')
    levels = int(config.get(feedname, 'levels'))

    feeddir = feedmeparser.sub_tilde(feeddir)
    feeddir = os.path.join(feeddir, time.strftime("%m-%d-%a"))

    formats = config.get(feedname, 'formats').split(',')
    encoding = config.get(feedname, 'encoding')
    ascii = config.getboolean(feedname, 'ascii')
    skip_links = config.getboolean(feedname, 'skip_links')
    skip_link_pat = feedmeparser.get_config_multiline(config,
                                                      feedname,
                                                      'skip_link_pat')

    # Is this a feed we should only check occasionally?
    """Does this feed specify only gathering at certain times?
       If so, has such a time passed since the last time the
       cache file was written?
    """
    when = config.get(feedname, "when")
    if when and when != '' and last_time:
        if not falls_between(when, last_time, time.localtime()):
            print >>sys.stderr, "Skipping", feedname, "-- not", when
            return
        print >>sys.stderr, "Yes, it's time to feed:", when

    #encoding = config.get(feedname, 'encoding')

    print >>sys.stderr, "\n============\nfeedname:", feedname
    # Use underscores rather than spaces in the filename.
    feedfile = feedname.replace(" ", "_")
    # Also, make sure there are no colons (illegal in filenames):
    feedfile = feedfile.replace(":", "")
    print >>sys.stderr, "feedfile:", feedfile
    outdir = os.path.join(feeddir,  feedfile)
    print >>sys.stderr, "outdir:", outdir

    if cache == None:
        nocache = True
    else:
        nocache = (config.get(feedname, 'nocache') == 'true')
    if verbose and nocache:
        msglog.msg(feedname + ": Ignoring cache")

    downloaded_string ="\n<hr><i>(Downloaded by " + \
        feedmeparser.VersionString + ")</i>\n"

    # feedparser doesn't understand file:// URLs, so translate those
    # to a local file:
    if sitefeedurl.startswith('file://'):
        sitefeedurl = sitefeedurl[7:]

    # feedparser.parse() can throw unexplained errors like
    # "xml.sax._exceptions.SAXException: Read failed (no details available)"
    # which will kill our whole process, so guard against that.
    # Sadly, feedparser usually doesn't give any details about what went wrong.
    socket.setdefaulttimeout(100)
    try:
        print "Running: feedparser.parse(", sitefeedurl, ")"
        feed = feedparser.parse(sitefeedurl)

    # except xml.sax._exceptions.SAXException, e:
    except Exception, e:
        print "Couldn't parse feed: URL:", sitefeedurl
        print str(e)
        traceback.print_stack()
        return

    # feedparser has no error return! One way is to check len(feed.feed).
    if len(feed.feed) == 0:
        msglog.err("Can't read " + sitefeedurl)
        return
    # XXX Sometimes feeds die a few lines later getting feed.feed.title.
    # Here's a braindead guard against it -- but why isn't this
    # whole clause inside a try? It should be.
    if not 'title' in feed.feed:
        msglog.msg(sitefeedurl + " lacks a title!")
        feed.feed.title = '[' + feedname + ']'
        #return

    if not nocache:
        if sitefeedurl not in cache:
            cache[sitefeedurl] = []
        feedcache = cache[sitefeedurl]
        newfeedcache = []

    # suburls: mapping of URLs we've encountered to local URLs.
    # Any anchors (#anchor) will be discarded.
    # This is for sites like WorldWideWords that make many links
    # to the same page.
    suburls = []

    # indexstr is the contents of the index.html file.
    # Kept as a string until we know whether there are new, non-cached
    # stories so it's worth updating the copy on disk.
    # The stylesheet is for FeedViewer and shouldn't bother plucker etc.
    day = time.strftime("%a")
    indexstr = u"""<html>\n<head>
<meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\">
<title>%s: %s</title>
<link rel="stylesheet" type="text/css" title="Feeds" href="../../feeds.css"/>
</head>

<body>\n<h1>%s: %s: %s</h1>
\n""" % (day, feedname, day, feedname, feed.feed.title)

    if verbose:
        print >>sys.stderr, "********* Reading", sitefeedurl

    # A pattern to tell the user how to get to the next story: >->
    # We also might want to remove that pattern later, in case
    # a story wasn't successfully downloaded -- so make a
    # regexp that can match it.
    next_item_string =  '<br>\n<center><i><a href=\"#%d\">&gt;-&gt;</a></i></center>\n<br>\n'
    next_item_pattern = '<br>\n<center><i><a href=\"#[0-9]+\">&gt;-&gt;</a></i></center>\n<br>\n'

    # We'll increment itemnum as soon as we start showing entries,
    # so start it negative so anchor links will start at zero.
    itemnum = -1
    for item in feed.entries:
        try:
            #
            # Get the list of links (href) and a (hopefully) unique ID:
            # XXX Is href[] ever even used? Is this clause obsolete?
            #
            if 'links' in item:
                href = [i['href'].encode('utf-8') \
                            for i in item.links if i['rel'] == 'alternate']
            else:
                href = []

            if not 'id' in item:
                if len(href) > 0:
                    item.id = href[0]
                    if verbose:
                        msglog.msg("Using URL " + href[0] + " for ID.")
                else:
                    if verbose:
                        msglog.msg("Item in " + href[0] + " had no ID or URL.")
                    next  # or return?

            # Does the link match a pattern we're skipping?
            if skip_link_pat:
                skipping = False
                for spat in skip_link_pat:
                    if re.search(spat, item.link):
                        if verbose:
                            print >>sys.stderr, "Skipping", item.link, \
                                "because it matches", spat
                        skipping = True
                        break
                if skipping:
                    continue

            # Filter out file types known not to work
            # XXX Only mp3 for now. Obviously, make this more general.
            # Wish we could do this using the server's type rather than
            # file extension!
            if item.link.endswith("mp3"):
                print >>sys.stderr, "Filtering out mp3 link", item.link
                continue

            # Make sure ids don't have named anchors appended:
            anchor_index = item.id.rfind('#')
            if anchor_index >= 0:
                anchor = item.id[anchor_index:]
                item.id = item.id[0:anchor_index]
            else:
                anchor = ""

            # See if we've already seen this page:
            try:
                pagenum = suburls.index(item.id)
                # We've already seen a link to this URL. It's probably
                # a link to a different named anchor within the same file.
            except ValueError:
                # Haven't seen it before. But is it in the cache already?
                if not nocache:
                    # We want it in the cache, whether it's new or not:
                    newfeedcache.append(item.id)
                    if item.id in feedcache:
                        if verbose:
                            msglog.msg(item.id + " already cached -- skipping")
                        continue

                # Add it to the cache and suburls.
                suburls.append(item.id)
                pagenum = len(suburls) - 1

            itemnum += 1
            if verbose:
                print >>sys.stderr, "\nItem:", item.title.encode('utf-8',
                                                                 'replace')

            # Now itemnum is the number of the entry on the index page;
            # pagenum is the html file of the subentry, e.g. 3.html.

            # Make the parent directory if we haven't already
            if not os.access(outdir, os.W_OK):
                if verbose:
                    print >>sys.stderr, "Making", outdir
                os.makedirs(outdir)

            if 'author' in item:
                author = item.author
            else:
                author = None

            #
            # Follow the link and make a file for it:
            #
            if levels > 1 :        # Normal multi-level site
                try :    # Try to trap keyboard interrupts, + others
                    # For the sub-pages, we're getting HTML, not RSS.
                    # Nobody seems to have RSS pointing to RSS.
                    parser = feedmeparser.FeedmeHTMLParser(config, feedname)
                    fnam = str(pagenum) + ".html"
                    parser.fetch_url(item.link,
                                     outdir, fnam,
                                     item.title, author,
                      "<center><a href=\"%d.html\">&gt;-%d-&gt;</a></center>" \
                                         % (itemnum, itemnum))

                except feedmeparser.NoContentError as e:
                    # fetch_url didn't manage to get the page or write a file.
                    # So don't increment pagenum or itemnum for the next story.
                    msglog.warn("Didn't find any content on " + item.link
                                + ": " + str(e))
                    # It is so annoying needing to repeat these
                    # lines every time! Isn't there a way I can define
                    # a subfunction that knows about this function's
                    # local variables?
                    itemnum -= 1
                    #pagenum -= 1
                    suburls.remove(item.id)

                    # Include a note in the indexstr
                    indexstr += '<p>No content for <a href="%s">%s</a>\n' \
                                % (item.link, item.title)
                    continue

                # Catch timeouts.
                # If we get a timeout on a story,
                # we should assume the whole site has gone down,
                # and skip over the rest of the site.
                # In Python 2.6, instead of raising socket.timeout
                # a timeout will raise urllib2.URLerror with
                # e.reason set to socket.timeout. 
                except socket.timeout, e:
                    errmsg = "Socket.timeout error on title "
                    errmsg += item.title.encode('utf-8', 'replace')
                    errmsg += "\n"
                    errmsg += "Breaking -- hopefully we'll write index.html"
                    msglog.err(errmsg)
                    if config.get(feedname, 'continue_on_timeout') == 'true':
                        continue
                    break
 
                # Handle timeouts in Python 2.6
                except urllib2.URLError, e:
                    if isinstance(e.reason, socket.timeout):
                        errmsg = "URLError Socket.timeout on title "
                        errmsg += item.title.encode('utf-8', 'replace')
                        errmsg += "\n"
                        errmsg += "Breaking -- hopefully we'll write index.html"
                        msglog.err(errmsg)
                        indexstr += "<p>" + errmsg
                        if config.get(feedname,
                                      'continue_on_timeout') == 'true':
                            continue
                        break

                    # Some other type of URLError.
                    errmsg = 'URLError on <a href="%s">%s</a><br>\n%s<br>\n' \
                             % (item.link, item.link, str(e))
                    msglog.err(errmsg)
                    indexstr += "<p><b>" + errmsg + "</b>"
                    continue

                except KeyboardInterrupt:
                    response = handle_keyboard_interrupt("""
*** Caught keyboard interrupt reading a story! ***\n
Options:
q: Quit
c: Continue trying to read this story
s: Skip to next story
n: Skip to next site

Which (default = s): """)
                    if response[0] == 'n' :      # next site
                        # XXX We should write an index.html here
                        # with anything we've gotten so far.
                        # Ideally we'd break out of the
                        # for item in feed.entries : loop.
                        # Wonder if there's a way to do that in python?
                        # Failing that, and hoping it's the only
                        # enclosing loop:
                        print "Breaking -- hopefully we'll write an index.html"
                        break
                        #return
                    elif response[0] != 'c' :    # next story (default)
                        continue
                    # If the response was 'c', we continue and just
                    # ignore the interrupt.

                except (IOError, urllib2.HTTPError) as e:
                    # Collect info about what went wrong:
                    errmsg = "Couldn't read " + item.link + "\n"
                    #errmsg += "Title: " + item.title.encode('utf-8', 'replace')
                    if verbose:
                        #errmsg += "Item summary was:\n------\n"
                        #errmsg += item.summary + "\n------\n"
                        errmsg += str(e) + '<br>\n'
                        #errmsg += str(sys.exc_info()[0]) + '<br>\n'
                        #errmsg += str(sys.exc_info()[1]) + '<br>\n'
                        #errmsg += traceback.format_exc(sys.exc_info()[2])

                    if verbose:
                        print >>sys.stderr, "=============="
                    msglog.err("IO or HTTP error: " + errmsg)
                    if verbose:
                        print >>sys.stderr, "=============="

                    itemnum -= 1
                    suburls.remove(item.id)

                    #raise  # so this entry won't get stored or cached

                    continue   # Move on to next story

                except ValueError, e:
                    # urllib2 is supposed to throw a urllib2.URLError for
                    # "unknown url type", but in practice it throws ValueError.
                    # See this e.g. for doubleclick ad links in the latimes
                    # that have no spec, e.g. //ad.doubleclick.net/...
                    # Unfortunately it seems to happen in other cases too,
                    # so there's no way to separate out the urllib2 ones
                    # except by string: str(sys.exc_info()[1]) starts with
                    # "unknown url type:"
                    errmsg = "ValueError on title "
                    errmsg += item.title.encode('utf-8', 'replace')
                    errmsg += "\n"
                    # print >>sys.stderr, errmsg
                    # msglog.err will print it, no need to print it again.
                    if str(sys.exc_info()[1]).startswith("unknown url type:"):
                        # Don't show stack trace for unknown URL types,
                        # since it's a known error.
                        errmsg += str(sys.exc_info()[1]) + " - couldn't load\n"
                        msglog.warn(errmsg)
                    else:
                        errmsg += "ValueError on url " + item.link + "\n"
                        errmsg += traceback.format_exc(sys.exc_info()[2])
                        msglog.err(errmsg)

                    itemnum -= 1
                    suburls.remove(item.id)
                    continue

                except Exception as e:
                    # An unknown error, so report it complete with traceback.
                    errmsg = "Unknown error reading " + item.link + "\n"
                    errmsg += "Title: " + item.title.encode('utf-8', 'replace')
                    if verbose:
                        errmsg += "\nItem summary was:\n------\n"
                        errmsg += item.summary + "\n------\n"
                        errmsg += str(e) + '<br>\n'
                        errmsg += str(sys.exc_info()[0]) + '<br>\n'
                        errmsg += str(sys.exc_info()[1]) + '<br>\n'
                        errmsg += traceback.format_exc(sys.exc_info()[2])

                    if verbose:
                        print >>sys.stderr, "=============="
                    msglog.err("Unknown error: " + errmsg)
                    if verbose:
                        print >>sys.stderr, "=============="

                    # Are we sure we didn't get anything?
                    # Should we decrement itemnum, etc. ?
                    continue   # Move on to next story, ensure we get index

            # Done with if levels > 1 clause
            
            if not 'published_parsed' in item:
                if 'updated_parsed' in item:
                    item.published_parsed = item.updated_parsed
                else:
                    item.published_parsed = time.gmtime()

            # Plucker named anchors don't work unless preceded by a <p>
     # http://www.mail-archive.com/plucker-list@rubberchicken.org/msg07314.html
            indexstr += "<p><a name=\"%d\">&nbsp;</a>" % itemnum

            # Make sure the link is at least some minimum width.
            # This is for viewers that have special areas defined on the
            # screen, e.g. areas for paging up/down or adjusting brightness.
            minwidth = config.getint(feedname, 'min_width')
            if len(item.title) < minwidth:
                #item.title += '&nbsp;' * (minwidth - len(item.title) - 2) + '__'
                item.title += '. ' * (minwidth - len(item.title)) + '__'

            if levels > 1:
                itemlink = '<a href=\"' + fnam + anchor + '\">'
                indexstr += itemlink + '<b>' + item.title + '</b></a>\n'
            else:
                # For a single-level site, don't put links over each entry.
                if skip_links:
                    itemlink = None
                    indexstr += "\n<b>" + item.title + "</b>\n"
                else:
                    itemlink = '<a href=\"' + item.link + '\">'
                    indexstr += "\n" + itemlink + item.title + "</a>\n"

            # Under the title, add a link to jump to the next entry.
            # If it's the last entry, we'll change it to "[end]" later.
            indexstr += next_item_string % (itemnum+1)

            # Add either the content or the summary:
            if 'summary_detail' in item:
                content = item.summary_detail.value + "\n"
            elif 'content' in item:
                content = item.content[0].value + "\n"
            else:
                content = "[No content]"

            # There's an increasing trend to load up RSS pages with images.
            # Try to remove them, as well as any links that contain
            # only an image.
            if config.getboolean(feedname, 'skip_images'):
                content = re.sub('<a [^>]*href=.*> *<img .*?></a>', '', content)
                content = re.sub('<img .*?>', '', content)
            # But if we're not skipping images, then we need to rewrite
            # image URLs to the local URLs we would have created in
            # feedmeparser.parse().
            else:
                if content.strip():
                    content = parser.rewrite_images(content)

            # Try to get rid of embedded links if skip_links is true:
            if skip_links:
                content = re.sub('<a href=.*>(.*?)</a>', '\\1', content)
            # If we're keeping links, don't keep empty ones:
            else:
                content = re.sub('<a  [^>]*href=.*> *</a>', '', content)

            # Skip any text specified in index_skip_pat.
            # Some sites (*cough* Pro Publica *cough*) do weird things
            # like putting huge <style> sections in the RSS.
            index_skip_pats = feedmeparser.get_config_multiline(config,
                                                      feedname,
                                                      'index_skip_pat')
            for pat in index_skip_pats:
                content = re.sub(pat, '', content)
                author = re.sub(pat, '', author)

            indexstr += content

            if author:
                indexstr += "\n<br><i>By: " + author + "</i><br>"

            # After the content, add another link to the title,
            # in case the user wants to click through after reading
            # the content:
            sublen = 16
            if len(item.title) > sublen:
                # Truncate the title to sublen characters, and
                # temove any HTML tags, otherwise we'll likely have
                # tags like <i> that open but don't close
                short_title = re.sub('<.*?>', '', item.title[0:sublen]) \
                    + "..."

            else:
                short_title = item.title
            if itemlink:
                indexstr += "\n<br>[[" + itemlink + short_title + "</a>]]\n\n"

        # If there was an error parsing this entry, we won't save
        # a file so decrement the itemnum and loop to the next entry.
        except KeyboardInterrupt:
            sys.stderr.flush()
            response = handle_keyboard_interrupt("""
*** Caught keyboard interrupt while finishing a site! ***\n
Options:
q: Quit
c: Continue trying to finish this site
n: Skip to next site

Which (default = n): """)
            if response[0] == 'c':
                continue
            if response[0] == 'q':
                sys.exit(1)
            # Default is to skip to the next site:
            return
        except Exception, e :    # probably an HTTPError, bad URL
            itemnum -= 1
            if verbose:
                print >>sys.stderr, "Skipping item",
                if 'link' in item:
                    print >>sys.stderr, item.link.encode('utf-8')
                else:
                    print >>sys.stderr, "item has no link! item =", item
                print >>sys.stderr, "error was", str(e).encode('utf-8')

                print >>sys.stderr, str(sys.exc_info()[0])
                print >>sys.stderr, str(sys.exc_info()[1])
                print >>sys.stderr, traceback.format_exc(sys.exc_info()[2])

    # Only write the index.html file if there was content that
    # wasn't already in the cache.
    if itemnum >= 0 or nocache:
        # If the RSS page ended with a story we didn't include,
        # either because it's old or because it had no content we could show,
        # we'll have an extra ">->" line. Try to remove it.
        # (Ugh, python re has no cleaner way of getting the last match.)
        m = None
        for m in re.finditer(next_item_pattern, indexstr):
            pass
        if m:
            print >>sys.stderr, "Removing final next-item pattern"
            indexstr = indexstr[:m.start()] \
                + "<br>\n<center><i>[end]</i></center>\n<br>\n" \
                + indexstr[m.end():]
        
        indexfile = os.path.join(outdir, "index.html")
        if verbose:
            print  >>sys.stderr, "Writing", indexfile
        index = open(indexfile, "w")
        if ascii:
            index.write(feedmeparser.output_encode(indexstr, 'ascii'))
        else:
            index.write(feedmeparser.output_encode(indexstr, encoding))

        # Before the downloaded string, insert a final named anchor.
        # On some sites we get a bug where we accidentally write a >>
        # when there are really no further stories. So give it a
        # place to go. Though if the removal of the final >->
        # succeeded, this should no longer be needed.
        index.write("<p><a name=\"%d\">&nbsp;</a>\n" % (itemnum+1))

        index.write(downloaded_string)
        index.write("\n</body>\n</html>\n")
        index.close()

        ####################################################
        # Generate the output files
        #
        if 'plucker' in formats:
            make_plucker_file(indexfile, feedname, levels, ascii)
        if 'fb2' in formats:
            make_fb2_file(indexfile, feedname, levels, ascii)
        if 'epub' in formats:
            make_epub_file(indexfile, feedname, levels, ascii)

        #
        # All done. Update the cache file.
        #
        if not nocache:
            if verbose:
                print >>sys.stderr, feedname, ": Updating cache file"
            # Dump the new cache, not the old one:
            # XXX Find out how long this is taking.
            # XXX Should we split the cache into per site?
            t = time.time()
            cache[sitefeedurl] = newfeedcache
            cPickle.dump(cache, open(cachefile, 'w'))
            print >>sys.stderr, "Writing cache took", time.time() - t, "seconds"
        elif verbose:
            print >>sys.stderr, feedname, ": Not updating cache file"

    else:
        print >>sys.stderr, feedname, ": no new content"

        # We may have made the directory. If so, remove it:
        # if there's no index file then there's no way to access anything there.
        if os.path.exists(outdir):
            print >>sys.stderr, "Removing directory", outdir
            shutil.rmtree(outdir)

#
# Find the cache file and load it, but don't parse yet
#
def init_cache():
    #
    # Load the cache file
    #
    if 'XDG_CACHE_HOME' in os.environ:
        cachefile = os.path.join(os.environ['XDG_CACHE_HOME'],
                                 'feedme', 'feedme.dat')
    else:
        cachefile = os.path.join(feedmeparser.sub_tilde('~/.cache'),
                                 'feedme', 'feedme.dat')

    if not os.path.exists(cachefile):
        dirname = os.path.dirname(cachefile)
        if not os.path.exists(dirname):
            os.makedirs(dirname)
        cache = {}
    elif not os.access(cachefile, os.W_OK):
        print >>sys.stderr, "Error: can't write cache file", cachefile
        sys.exit(1)
    else:
        # Make a backup of the cache file, in case something goes wrong:
        shutil.copy2(cachefile, cachefile + ".bak")
        cache = cPickle.load(open(cachefile))

    return cache, cachefile

#
# Main -- read the config file and loop over sites.
#
if __name__ == '__main__':
    from optparse import OptionParser

    usage = """%prog [site ...]
If no site is specified, feedme will update all the feeds in
~/.config/feedme.conf."""
    LongVersion = feedmeparser.VersionString + ": an RSS feed reader.\n\
Copyright 2011 by Akkana Peck; share and enjoy under the GPL v2 or later."

    optparser = OptionParser(usage=usage, version=LongVersion)
    optparser.add_option("-n", "--nocache",
                         action="store_true", dest="nocache",
                         help="Don't consult the cache, or update it")
    optparser.add_option("-s", "--show-sites",
                         action="store_true", dest="show_sites",
                         help="Show available sites")
    optparser.add_option("-l", "--log", metavar="logfile",
                         action="store", dest="log_file_name",
                         help="Save output to a log file")
    optparser.add_option("-c", "--config-help",
                         action="store_true", dest="config_help",
                         help="Print help on configuration files")
    (options, args) = optparser.parse_args()

    config = feedmeparser.read_config_file()

    msglog = MsgLog()

    sections = config.sections()

    if options.config_help:
        print LongVersion
        print ConfigHelp
        sys.exit(0)

    if options.show_sites:
        for feedname in sections:
            print feedname
        sys.exit(0)

    if options.nocache:
        cache = None
        cachefile = None
        last_time = None
    else:
        cache, cachefile = init_cache()
        # Figure out the last time we ran feedme.
        # We'll use this for feeds that only update at certain times.
        try:
            statbuf = os.stat(cachefile)
            last_time = statbuf.st_mtime
        except OSError :    # probably the cache is new and got File Not Found
            last_time = 0

    # logfilename = config.get('DEFAULT', 'logfile')
    # if logfilename:
    #     logfilename = feedmeparser.sub_tilde(logfilename)

    # Ignore the config file logfile setting, and log to feeds/LOG.
    # When we're finished, we'll move the file to inside the newly
    # created feed directory.

    feeddir = feedmeparser.sub_tilde(config.get('DEFAULT', 'dir'))
    if not os.path.exists(feeddir):
        os.makedirs(feeddir)
    logfilename = os.path.join(feeddir, 'LOG')

    # Set up a tee to the log file, and redirect stderr there:
    print "teeing output to", logfilename
    stderrsav = sys.stderr
    outputlog = open(logfilename, "w", buffering=1)
    sys.stderr = tee(stderrsav, outputlog)

    try:
        if len(args) == 0:
            for feedname in sections:
                # This can hang if feedparser hangs parsing the initial RSS.
                # So give the user a chance to ^C out of one feed
                # without stopping the whole run:
                try:
                    get_feed(feedname, config, cache, cachefile,
                             last_time, msglog)
                except KeyboardInterrupt:
                    print >>sys.stderr, "Interrupt! Skipping feed", feedname
                    handle_keyboard_interrupt("Type q to quit, anything else to skip to next feed: ")
                    # We don't actually have to check the return value;
                    # handle_keyboard_interrupt will quit if the user types q.

        #sys.exit(1)
        else:
            for arg in args:
                print >>sys.stderr, 'Getting feed for', arg
                get_feed(arg, config, cache, cachefile, last_time, msglog)

    # This causes a lot of premature exits. Not sure why we end up
    # here rather than in the inner KeyboardInterrupt section.
    except KeyboardInterrupt:
        print >>sys.stderr, "Caught keyboard interrupt at the wrong time!"
        print traceback.format_exc(sys.exc_info()[2])
        #sys.exit(1)
    except OSError, e:
        print >>sys.stderr, "Caught an OSError"
        print >>sys.stderr, e
        #sys.exit(e.errno)

    # Dump any errors we encountered.
    msgs = msglog.get_msgs()
    if msgs:
        print >>sys.stderr, "\n===== Messages ===="
        print >>sys.stderr, msgs.encode('utf-8', 'backslashreplace')
    msgs = msglog.get_errs()
    if msgs:
        print >>sys.stderr, "\n====== Errors ====="
        print >>sys.stderr, msgs.encode('utf-8', 'backslashreplace')

    try:
        # Now we're done. It's time to move the log file into its final place.
        datestr = time.strftime("%m-%d-%a")
        datedir = os.path.join(feeddir, datestr)
        os.rename(logfilename,
                  os.path.join(datedir, 'LOG'))

        # and make a manifest listing all the files we downloaded.
        # This will be used remotely, so we don't want the local
        # path in it; everything is relative to this directory.
        # XXX Temporarily, we'll only do it for me, so Dave doesn't
        # get MANIFEST files cluttering up his directories.
        # MANIFEST should end with .EOF. on a line by itself
        # to avoid race conditions where the fetcher thinks it's
        # read the manifest while the file is still being written.
        if 'akkana' in datedir or 'shallowsky' in datedir:
            discardchars = len(datedir)
            print "Discarding", discardchars, "characters"
            print >>sys.stderr, "Writing MANIFEST"
            manifest = open(os.path.join(datedir, 'MANIFEST'), 'w')
            for root, dirs, files in os.walk(datedir):
                shortroot = root[discardchars+1:]
                if shortroot:
                    print >>manifest, shortroot + '/'
                for f in files:
                    print >>manifest, posixpath.join(shortroot, f)
            print >>sys.stderr, "Trying to write the special code at end of MANIFEST"
            print >>manifest, ".EOF."
            print >>sys.stderr, "Wrote the special code"
            manifest.close()
    except OSError, e:
        print >>sys.stderr, "Couldn't move LOG or create MANIFEST"
        print e

    # Clean up old directories:
    clean_up(config)

